# Theorex

Theorex is an advanced AI-powered document analysis and context aware question-answering system facilitaing you to get specific and efficient answer for your questions/queries relevant to the context and not the global Knowledge base. It empowers users to upload documents and interact with them conversationally using natural language queries.

---

## Project Structure

```
Theorex/
â”œâ”€â”€ backend/            # FastAPI backend serving the API and RAG pipeline
â”‚   â”œâ”€â”€ main.py           # FastAPI application entry point
â”‚   â”œâ”€â”€ api.py            # API routes (upload, ask, reset)
â”‚   â”œâ”€â”€ rag_pipeline.py      # Core Retrieval-Augmented Generation (RAG) logic
â”‚   â”œâ”€â”€ CRLLM.py      # a custom made runnable which behaves as an text generation LLM required by project Logic
â”‚   â”œâ”€â”€ file_utilitiess.py          # Utility functions (embedding, text parsing)
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚
â”œâ”€â”€ frontend/             # React frontend providing the user interface
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚â”€â”€ assets/       # Images (e.g., Theorex logo, background)
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadBox.jsx         # File upload component
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatBox.jsx         # Chat UI component
â”‚   â”‚   â”œâ”€â”€ App.jsx          # Main React component
â”‚   â”‚   â”œâ”€â”€ App.css          # Styling (with animations)
â”œ   â”œâ”€â”€ api.js          # API routes (upload, ask, reset)
â”‚   â”œâ”€â”€ package.json            # Frontend dependencies
â”‚
â””â”€â”€ README.md              # Project documentation
```

---

## Features

- Upload PDF documents for analysis
- Chat-based query system powered by LLMs
- Retrieval-Augmented Generation (RAG) pipeline for contextual answers
- Responsive UI with smooth transitions
- Separate Reset and New Document workflows

---

## Tech Stack

### Frontend

- **React** (UI Framework)
- **CSS Animations** (Logo transitions, background zoom effects)
- Axios (API calls)

### Backend

- **FastAPI** (REST API framework)
- **LangChain** (RAG implementation)
- **OpenAI API** (LLM for answering queries)
- **FAISS** (for semantic search and embeddings)

---

## Architecture Overview

### Retrieval-Augmented Generation (RAG)

1. **Document Upload**

   - User uploads a PDF document.
   - Backend parses and splits the text into chunks.

2. **Vectorization**

   - Each chunk is embedded using OpenAI embeddings.
   - Stored in a FAISS vector database for similarity search.

3. **Query Handling**

   - User enters a natural language query.
   - Backend retrieves relevant document chunks using similarity search.

4. **LLM Response**

   - Retrieved chunks are sent to HUGGINGFACE HuggingFaceH4/zephyr-7b-beta model with the user query.
   - LLM generates a contextualized answer.

5. **Chat UI**

   - Frontend displays user query and AI response.

```
User Query --> RAG Pipeline --> LLM --> Response
```

### Data Flow

```
Frontend (React)
   â†“
Backend API (FastAPI)
   â†“
RAG Engine (LangChain + FAISS)
   â†“
HUGGINGFACE LLM (HuggingFaceH4/zephyr-7b-beta)
   â†“
Frontend Response
```

---

## Installation & Running

### Backend (FastAPI)

```bash
cd backend
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload
```

### Frontend (React)

```bash
cd frontend
npm install
npm start
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

---

## ğŸ“„ API Endpoints

| Method | Endpoint  | Description            |
| ------ | --------- | ---------------------- |
| POST   | `/upload` | Uploads a PDF document |
| POST   | `/ask`    | Sends a user query     |
| POST   | `/reset`  | Clears session memory  |

---

## ğŸ“ Environment Variables

Backend requires the following environment variables:

```env
HUGGINGFACEHUB_API_TOKEN=your_openai_api_key
```

---

## ğŸ‘¨â€ğŸ’» Developers Note

- LLM integration via HUGGINGFACE API.
- FAISS is used for in-memory vector storage.
- Designed for modularity: swap Zephyr Mistral 7b with another LLM if needed.
- Implented leveraging Langchains for swift integration and empoweing project with the chain pipelining
---

## ğŸ“Œ Future Improvements

- Multi-document support
- Auth and user sessions

---

## ğŸ·ï¸ License

This project is licensed under the MIT License.

